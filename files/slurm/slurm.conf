# slurm.conf file generated by configurator easy.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ControlMachine=head
ControlAddr=10.0.0.1
#
#MailProg=/bin/mail
MpiDefault=none
#MpiParams=ports=#-#
ProctrackType=proctrack/cgroup
ReturnToService=1
SlurmctldPidFile=/var/run/slurmctld.pid
#SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
#SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser=slurm
#SlurmdUser=root
StateSaveLocation=/var/spool/slurmctld
SwitchType=switch/none
TaskPlugin=task/affinity
#
#
# TIMERS
KillWait=30
MinJobAge=300
SlurmctldTimeout=300
SlurmdTimeout=300
Waittime=0
InactiveLimit=0
#
#
# SCHEDULING
FastSchedule=1
SchedulerType=sched/backfill
SelectType=select/cons_res
SelectTypeParameters=CR_Core_Memory
DefMemPerCPU=4096
MaxMemPerCPU=10752
#
#
# LOGGING AND ACCOUNTING
AccountingStorageType=accounting_storage/none
ClusterName=lenovo
#JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/none
#SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurmctld.log
#SlurmdDebug=3
SlurmdLogFile=/var/log/slurmd.log


# CPU NODES

NodeName=cpu1     NodeAddr=10.0.0.11      RealMemory=192905 Sockets=2 CoresPerSocket=12 ThreadsPerCore=2 Gres=mic
NodeName=cpu[2-4] NodeAddr=10.0.0.[12-14] RealMemory=386286 Sockets=2 CoresPerSocket=12 ThreadsPerCore=2 Gres=mic
NodeName=cpu[5-8] NodeAddr=10.0.0.[15-18] RealMemory=192905 Sockets=2 CoresPerSocket=12 ThreadsPerCore=2 Gres=mic

# GPU NODES
NodeName=gpu[1-2] NodeAddr=10.0.0.[51-52] RealMemory=192763 Sockets=2 CoresPerSocket=8  ThreadsPerCore=2 Gres=gpu:2

# PRIORITY
PriorityType=priority/multifactor
PriorityWeightAge=0
PriorityWeightFairshare=1000
PriorityWeightJobSize=0
PriorityWeightPartition=0
PriorityWeightQOS=0

# PREEMPTION
PreemptType=preempt/partition_prio
PreemptMode=requeue

# PARTITIONS / QUEUES

#PartitionName=matlab Default=NO MinNodes=1 AllowGroups=ALL DisableRootJobs=NO RootOnly=NO Hidden=NO Shared=NO GraceTime=0 PreemptMode=OFF ReqResv=NO AllowAccounts=ALL AllowQos=ALL LLN=NO ExclusiveUser=NO PriorityJobFactor=1 PriorityTier=300 OverSubscribe=NO State=UP Nodes=node100
PartitionName=tesla-long Default=NO MinNodes=1 MaxTime=INFINITE AllowGroups=ALL DisableRootJobs=NO RootOnly=NO Hidden=NO Shared=NO GraceTime=0 PreemptMode=OFF ReqResv=NO AllowAccounts=ALL AllowQos=ALL LLN=NO ExclusiveUser=NO PriorityJobFactor=1 PriorityTier=200 OverSubscribe=NO State=UP Nodes=gpu[1-2]
PartitionName=tesla-short Default=NO MinNodes=1 MaxTime=04:00:00 AllowGroups=ALL DisableRootJobs=NO RootOnly=NO Hidden=NO Shared=NO GraceTime=0 PreemptMode=OFF ReqResv=NO AllowAccounts=ALL AllowQos=ALL LLN=NO ExclusiveUser=NO PriorityJobFactor=1 PriorityTier=200 OverSubscribe=NO State=UP Nodes=node[1-2]
PartitionName=cpu-short Default=YES MaxTime=7-00:00:00 AllowGroups=ALL DisableRootJobs=NO RootOnly=NO Hidden=NO Shared=NO GraceTime=0 PreemptMode=OFF ReqResv=NO AllowAccounts=ALL AllowQos=ALL LLN=NO ExclusiveUser=NO PriorityJobFactor=1 PriorityTier=200 OverSubscribe=NO State=UP Nodes=cpu[1-8]
PartitionName=cpu-long Default=NO MaxTime=INFINITE AllowGroups=ALL DisableRootJobs=NO RootOnly=NO Hidden=NO Shared=NO GraceTime=0 PreemptMode=OFF ReqResv=NO AllowAccounts=ALL AllowQos=ALL LLN=NO ExclusiveUser=NO PriorityJobFactor=1 PriorityTier=200 OverSubscribe=NO State=UP Nodes=cpu[1-8]
